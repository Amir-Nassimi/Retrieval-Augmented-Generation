{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dc958e1-ffbc-4fd2-acf7-4b5d1d8de6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4f3be6-cf0c-4e5c-9142-d8ff0a5290ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a.nasimi\\Anaconda3\\envs\\LLM_Env\\lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize GPU device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = 'gpt2'  # Replace with your model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07cb9079-df02-4f9e-bb48-ed28bbe45d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]\n",
    "\n",
    "# Prepare the prompt for GPT-2\n",
    "conversation = \"\"\n",
    "for message in messages:\n",
    "    if isinstance(message, SystemMessage):\n",
    "        conversation += f\"System: {message.content}\\n\"\n",
    "    elif isinstance(message, HumanMessage):\n",
    "        conversation += f\"Human: {message.content}\\n\"\n",
    "    elif isinstance(message, AIMessage):\n",
    "        conversation += f\"AI: {message.content}\\n\"\n",
    "\n",
    "# Add the final human message for which we want a response\n",
    "conversation += \"AI:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae36e40-c278-4706-a48b-af1a6bdf0b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI Response: System: You are a helpful assistant.\n",
      "Human: Hi AI, how are you today?\n",
      "AI: I'm great thank you. How can I help you?\n",
      "Human: I'd like to understand string theory.\n",
      "AI: I'm sorry, but I'm not sure what string theory is.\n",
      "Human: I'm sorry, but I'm not sure what string theory is.\n",
      "Human: I'm sorry, but I'm not sure what string theory is.\n",
      "Human: I'm sorry, but I'm not sure what string theory is.\n",
      "Human: I'm sorry, but I'm not sure what string theory is.\n",
      "Human: I'm sorry, but I'm not sure what string theory is.\n",
      "Human:\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids, \n",
    "        max_length=150, \n",
    "        num_return_sequences=1, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output and return as a string\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "response = generate_response(conversation)\n",
    "\n",
    "print(\"AI Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83b317-73b5-4bbe-a8f7-6c1cfb131259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
