{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3f98f3a-6ecc-40b3-93ec-c9818a9e0c63",
   "metadata": {},
   "source": [
    "# Read Me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3c578-b26e-4e6b-902f-0feb98650f64",
   "metadata": {},
   "source": [
    "GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective.\n",
    "\n",
    "The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released.\n",
    "\n",
    "note: Because large-scale language models like GPT-2 do not distinguish fact from fiction, we donâ€™t support use-cases that require the generated text to be true.\r\n",
    "\r\n",
    "Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so we do not recommend that they be deployed into systems that interact with humans unless the deployers first carry out a study of biases relevant to the intended use-case. We found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attribut. Due these explanation and its huge size, this model has never been implemented!\n",
    "\n",
    "- Amirali Nassimies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd7f75-b337-4a49-a0f7-4eb3262df009",
   "metadata": {},
   "source": [
    "# Load Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7f0b5-5068-47e9-b79c-94daaef6519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147f0b6-a847-4972-b058-0196c8db15e6",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1716a-5d68-422b-a13a-599006216c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-large\"  # You can also use \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc35da-79ef-49da-9d5a-1f50717f19b1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110269e-866d-4a72-95e1-f2a3720e47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=200, num_return_sequences=1)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
