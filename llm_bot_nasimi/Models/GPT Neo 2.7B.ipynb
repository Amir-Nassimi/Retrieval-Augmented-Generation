{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5118419f-e03a-41d9-a476-2fe34ac9d2f4",
   "metadata": {},
   "source": [
    "# Read Me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba35104-82ae-4b34-866b-f99017a1b0d3",
   "metadata": {},
   "source": [
    "The Purpose of this source code is to find out about the details of implementation and the way of utilizing the GPT-Neo 2.7B which is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. \n",
    "\n",
    "This model was trained for 420 billion tokens over 400,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss. In this way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt. The Lambda acc is 62.22% for this model.\n",
    "\n",
    "note: You can also use EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B\n",
    "\n",
    "How ever due to the size of this model and since ti was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending on your usecase GPT-Neo may produce socially unacceptable tex; ergo, it has not been implemented.\n",
    "\n",
    "- Amirali Nassimi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8abe-6f1d-4c29-bda9-71a78381b0c5",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82575e9-e2dd-4c8d-b919-26df84f21f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac073e-6b0a-44f7-bc22-eeade4b4e286",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a8351-bbdf-4779-8755-83b40354fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-neo-2.7B\"\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9bda9b-8a68-4869-b841-d871c61d03d2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9f866-7397-44c9-86d5-d81c7cf834df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
