{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2fd349-d695-441a-9f8d-249609ed938e",
   "metadata": {},
   "source": [
    "# ReadMe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c5d28-4317-466c-83f8-a1e5eb6b734d",
   "metadata": {},
   "source": [
    "The Purpose of this source code is to find out about the details of implementation and the way of utilizing the gpt-j-6B Model. How ever due to the size of this model and since it is not intended for deployment without fine-tuning, supervision, and/or moderation, it is not a in itself a product and cannot be used for human-facing interactions; so it was never implemented! \n",
    "\n",
    "note: the model may generate harmful or offensive text. Please evaluate the risks associated with your particular use case.It B was trained on an English-language only dataset, and is thus not suitable for translation or generating text in other language\n",
    "\n",
    "GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. \"GPT-J\" refers to the class of model, while \"6B\" represents the number of trainable parameters. The core functionality of GPT-J is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work. When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most \"accurate\" text. Never depend upon GPT-J to produce factually accurate output.\n",
    "\n",
    "- Amirali Nassimi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a201606-665e-4512-b74a-f36439c9fc36",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43811246-8b90-4b6d-ae9d-05e5563c0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTJForCausalLM, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2b0920-050c-4d8c-9122-1b3eaefa4281",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6250475-ee6f-4bc2-9b70-c19c8f5d5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "model = GPTJForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ca13a-5e9f-48cc-a68b-b079266c99c1",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55a833-9ee7-4442-af8c-95674b64086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
